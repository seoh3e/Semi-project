# dashboard/app.py

import json
import re
from pathlib import Path
from typing import Tuple

import pandas as pd
import streamlit as st


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Darkweb Leak Intelligence Dashboard",
    layout="wide",
)

st.markdown(
    """
    <style>
    .main > div {
        padding-top: 1rem;
        padding-bottom: 2rem;
    }
    section[data-testid="stSidebar"] > div {
        padding-top: 1rem;
    }
    .small-text {
        font-size: 0.85rem;
        color: #888888;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ë°ì´í„° íŒŒì¼ ìœ„ì¹˜ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€: /data/leak_records.csv)
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
CSV_PATH = DATA_DIR / "leak_records.csv"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=60)
def load_data() -> pd.DataFrame:
    """CSVì—ì„œ LeakRecord ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤."""
    if not CSV_PATH.exists():
        return pd.DataFrame()

    df = pd.read_csv(CSV_PATH)

    # ë‚ ì§œ ìºìŠ¤íŒ…
    for col in ["collected_at", "posted_at"]:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors="coerce")

    # volume ìˆ«ìí˜• ìºìŠ¤íŒ…
    if "estimated_volume" in df.columns:
        df["estimated_volume"] = pd.to_numeric(df["estimated_volume"], errors="coerce")

    # ë¬¸ìì—´ ì»¬ëŸ¼ NaN ì •ë¦¬
    for col in ["source", "confidence", "post_title", "target_service", "threat_claim", "domains", "leak_types"]:
        if col in df.columns:
            df[col] = df[col].fillna("").astype(str)

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ„í—˜ë„ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_risk_score(row: pd.Series) -> Tuple[int, str, str]:
    """
    confidence + estimated_volume ê¸°ë°˜ ìœ„í—˜ë„ ì ìˆ˜/ë¼ë²¨/ì•„ì´ì½˜ ê³„ì‚°.
    ì ìˆ˜(total), ë¼ë²¨, ìƒ‰ìƒì•„ì´ì½˜(ì´ëª¨ì§€) ë°˜í™˜.
    """
    conf = str(row.get("confidence", "")).lower()

    if conf == "high":
        base = 3
    elif conf == "medium":
        base = 2
    elif conf == "low":
        base = 1
    else:
        base = 1

    vol = row.get("estimated_volume", 0)
    if pd.isna(vol):
        vol = 0

    if vol >= 1_000_000:
        vol_score = 3
    elif vol >= 100_000:
        vol_score = 2
    elif vol > 0:
        vol_score = 1
    else:
        vol_score = 0

    total = base + vol_score  # 0 ~ 6

    if total >= 5:
        label = "ìœ„í—˜ë„ ë§¤ìš° ë†’ìŒ"
        color = "ğŸ”´"
    elif total >= 3:
        label = "ìœ„í—˜ë„ ë†’ìŒ"
        color = "ğŸŸ "
    elif total >= 2:
        label = "ìœ„í—˜ë„ ë³´í†µ"
        color = "ğŸŸ¡"
    else:
        label = "ìœ„í—˜ë„ ë‚®ìŒ"
        color = "ğŸŸ¢"

    return total, label, color


def add_risk_info(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì— risk_score / risk_label / risk_indicator ì»¬ëŸ¼ì„ ì¶”ê°€."""
    if df.empty:
        return df

    df = df.copy()
    scores = df.apply(compute_risk_score, axis=1, result_type="expand")
    df["risk_score"] = scores[0]
    df["risk_label"] = scores[1]
    df["risk_indicator"] = scores[2]
    return df


def split_csv_list_cell(value: str) -> list[str]:
    """'a, b, c' í˜•íƒœë¥¼ ['a','b','c']ë¡œ ì•ˆì „í•˜ê²Œ ë¶„ë¦¬."""
    if value is None:
        return []
    s = str(value).strip()
    if not s:
        return []
    return [t.strip() for t in s.split(",") if t.strip()]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    st.title("ğŸŒ™ Darkweb Leak Intelligence Dashboard")
    st.markdown(
        '<p class="small-text">'
        "í…”ë ˆê·¸ë¨ ê¸°ë°˜ ë‹¤í¬ì›¹ ìœ ì¶œ ì •ë³´ ìë™ ìˆ˜ì§‘ Â· ë¶„ì„ ëŒ€ì‹œë³´ë“œ"
        "</p>",
        unsafe_allow_html=True,
    )

    df = load_data()

    if st.button("ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        load_data.clear()
        st.experimental_rerun()

    if df.empty:
        st.warning("í˜„ì¬ data/leak_records.csv íŒŒì¼ì´ ì—†ê±°ë‚˜, ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    df = add_risk_info(df)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´ë“œë°”: ê²€ìƒ‰ Â· í•„í„° Â· ì •ë ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.sidebar.header("ê²€ìƒ‰ / í•„í„°")

    # 1) ê²€ìƒ‰
    st.sidebar.subheader("ê²€ìƒ‰(Search)")
    q_title = st.sidebar.text_input("ì œëª© ê²€ìƒ‰ (post_title)")
    q_domain = st.sidebar.text_input("ë„ë©”ì¸ ê²€ìƒ‰ (domains)")
    q_target = st.sidebar.text_input("íƒ€ê²Ÿ ì„œë¹„ìŠ¤ ê²€ìƒ‰ (target_service)")
    q_source = st.sidebar.text_input("ì†ŒìŠ¤/ì±„ë„ ê²€ìƒ‰ (source)")
    q_any = st.sidebar.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰ (ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œ)")

    df_filtered = df.copy()

    if q_title and "post_title" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["post_title"].astype(str).str.contains(q_title, case=False, na=False)
        ]

    if q_domain and "domains" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["domains"].astype(str).str.contains(q_domain, case=False, na=False)
        ]

    if q_target and "target_service" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["target_service"].astype(str).str.contains(q_target, case=False, na=False)
        ]

    if q_source and "source" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["source"].astype(str).str.contains(q_source, case=False, na=False)
        ]

    if q_any:
        text_cols = df_filtered.select_dtypes(include=["object"]).columns
        if len(text_cols) > 0:
            df_tmp = df_filtered.copy()
            df_tmp["_concat"] = df_tmp[text_cols].fillna("").astype(str).agg(" ".join, axis=1)
            df_filtered = df_tmp[df_tmp["_concat"].str.contains(q_any, case=False, na=False)].drop(columns=["_concat"])

    # 2) í•„í„°
    st.sidebar.subheader("í•„í„°(Filter)")

    # leak_types í•„í„°
    if "leak_types" in df.columns:
        all_leak_types: list[str] = []
        for v in df["leak_types"].dropna().astype(str):
            all_leak_types.extend(split_csv_list_cell(v))

        leak_type_values = sorted(set(all_leak_types))
        selected_leak_types = st.sidebar.multiselect("Leak Types", leak_type_values)

        if selected_leak_types:
            pattern = "|".join([re.escape(t) for t in selected_leak_types])
            df_filtered = df_filtered[
                df_filtered["leak_types"].astype(str).str.contains(pattern, na=False)
            ]

    # confidence í•„í„°
    if "confidence" in df.columns:
        confidence_values = sorted(df["confidence"].dropna().astype(str).unique().tolist())
        selected_confidence = st.sidebar.multiselect("Confidence", confidence_values)
        if selected_confidence:
            df_filtered = df_filtered[df_filtered["confidence"].astype(str).isin(selected_confidence)]

    # ë‚ ì§œ ë²”ìœ„ í•„í„° (collected_at / posted_at)
    date_field = None
    if "collected_at" in df.columns or "posted_at" in df.columns:
        st.sidebar.markdown("---")
        st.sidebar.write("ë‚ ì§œ ë²”ìœ„ í•„í„°")

        available_date_fields = []
        if "collected_at" in df.columns:
            available_date_fields.append("collected_at")
        if "posted_at" in df.columns:
            available_date_fields.append("posted_at")

        date_field = st.sidebar.selectbox("ê¸°ì¤€ ë‚ ì§œ ì»¬ëŸ¼", available_date_fields)

        min_date = df[date_field].min()
        max_date = df[date_field].max()

        if not (pd.isna(min_date) or pd.isna(max_date)):
            start_date, end_date = st.sidebar.date_input(
                f"{date_field} ë²”ìœ„",
                value=[min_date.date(), max_date.date()],
            )
            if start_date and end_date:
                mask = (df_filtered[date_field] >= pd.to_datetime(start_date)) & (
                    df_filtered[date_field] <= pd.to_datetime(end_date)
                )
                df_filtered = df_filtered[mask]

    # 3) ì •ë ¬
    st.sidebar.markdown("---")
    st.sidebar.subheader("ì •ë ¬(Sort)")

    sort_options = ["ì •ë ¬ ì—†ìŒ"]
    if "posted_at" in df.columns:
        sort_options.append("ìµœì‹ ìˆœ (posted_at desc)")
    if "collected_at" in df.columns:
        sort_options.append("ìµœì‹ ìˆœ (collected_at desc)")
    if "estimated_volume" in df.columns:
        sort_options.append("volume í° ìˆœ")
    if "source" in df.columns:
        sort_options.append("source ì•ŒíŒŒë²³ ìˆœ")
    sort_options.append("ìœ„í—˜ë„ ë†’ì€ ìˆœ (risk_score desc)")

    sort_key = st.sidebar.selectbox("ì •ë ¬ ê¸°ì¤€", sort_options)

    if sort_key == "ìµœì‹ ìˆœ (posted_at desc)" and "posted_at" in df_filtered.columns:
        df_filtered = df_filtered.sort_values("posted_at", ascending=False)
    elif sort_key == "ìµœì‹ ìˆœ (collected_at desc)" and "collected_at" in df_filtered.columns:
        df_filtered = df_filtered.sort_values("collected_at", ascending=False)
    elif sort_key == "volume í° ìˆœ" and "estimated_volume" in df_filtered.columns:
        df_filtered = df_filtered.sort_values("estimated_volume", ascending=False, na_position="last")
    elif sort_key == "source ì•ŒíŒŒë²³ ìˆœ" and "source" in df_filtered.columns:
        df_filtered = df_filtered.sort_values("source", ascending=True)
    elif sort_key == "ìœ„í—˜ë„ ë†’ì€ ìˆœ (risk_score desc)" and "risk_score" in df_filtered.columns:
        df_filtered = df_filtered.sort_values("risk_score", ascending=False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒë‹¨ KPI ì˜ì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì „ì²´ ë ˆì½”ë“œ ìˆ˜", len(df))
    with col2:
        st.metric("í˜„ì¬ í•„í„°ëœ ë ˆì½”ë“œ ìˆ˜", len(df_filtered))
    with col3:
        if "source" in df.columns:
            st.metric("ì†ŒìŠ¤(ì±„ë„) ê°œìˆ˜", df["source"].nunique())

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°í™” ì˜ì—­ (matplotlib ì—†ì´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("## ğŸ“ˆ í†µê³„ / ì‹œê°í™”")

    if df_filtered.empty:
        st.info("í˜„ì¬ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        col_g1, col_g2 = st.columns(2)

        with col_g1:
            # ë‚ ì§œë³„ ê±´ìˆ˜ ì¶”ì´: Streamlit line_chart ì‚¬ìš©
            if "posted_at" in df_filtered.columns or "collected_at" in df_filtered.columns:
                date_field_for_chart = (
                    date_field
                    if date_field in ["posted_at", "collected_at"]
                    else ("posted_at" if "posted_at" in df_filtered.columns else "collected_at")
                )

                df_time = df_filtered.copy()
                df_time[date_field_for_chart] = pd.to_datetime(df_time[date_field_for_chart], errors="coerce")
                df_time = df_time.dropna(subset=[date_field_for_chart])

                st.subheader("ë‚ ì§œë³„ ëˆ„ì¶œ ê±´ìˆ˜ ì¶”ì´")
                if not df_time.empty:
                    daily_counts = (
                        df_time.groupby(df_time[date_field_for_chart].dt.date)
                        .size()
                        .reset_index(name="count")
                    )
                    daily_counts = daily_counts.rename(columns={daily_counts.columns[0]: "date"}).set_index("date")
                    st.line_chart(daily_counts["count"])
                else:
                    st.write("ë‚ ì§œ ì •ë³´ê°€ ì—†ì–´ íŠ¸ë Œë“œ ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with col_g2:
            # ì±„ë„ë³„ ëˆ„ì¶œ ë¹„ì¤‘: Streamlit bar_chart ì‚¬ìš©
            if "source" in df_filtered.columns:
                st.subheader("ì±„ë„ë³„ ëˆ„ì¶œ ê±´ìˆ˜ (source ê¸°ì¤€)")
                channel_counts = df_filtered["source"].astype(str).value_counts()
                st.bar_chart(channel_counts)

        # Leak Types ë¹„ìœ¨: pie ëŒ€ì‹  bar_chartë¡œ ëŒ€ì²´ (matplotlib ì œê±°)
        st.subheader("Leak Types (count)")
        if "leak_types" in df_filtered.columns:
            all_types: list[str] = []
            for v in df_filtered["leak_types"].dropna().astype(str):
                all_types.extend(split_csv_list_cell(v))

            if all_types:
                type_counts = pd.Series(all_types).value_counts()
                st.bar_chart(type_counts)
            else:
                st.write("Leak Types ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.write("leak_types ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë ˆì½”ë“œ í…Œì´ë¸” + ìƒì„¸ ë³´ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("## ğŸ“„ Leak Records")

    columns_for_table = []
    for col in [
        "id",
        "risk_indicator",
        "risk_label",
        "confidence",
        "estimated_volume",
        "post_title",
        "source",
        "domains",
        "leak_types",
        "posted_at",
    ]:
        if col in df_filtered.columns:
            columns_for_table.append(col)

    if not df_filtered.empty:
        st.dataframe(df_filtered[columns_for_table], use_container_width=True, hide_index=True)

    # ìƒì„¸ ë³´ê¸°ìš© key ê²°ì • (id ì—†ìœ¼ë©´ index ì‚¬ìš©)
    df_detail = df_filtered.copy()
    if "id" in df_detail.columns:
        record_ids = df_detail["id"].astype(str).tolist()
        id_label = "id"
    else:
        df_detail = df_detail.reset_index().rename(columns={"index": "_idx"})
        record_ids = df_detail["_idx"].astype(str).tolist()
        id_label = "index"

    if record_ids:
        selected_id = st.selectbox(f"ìƒì„¸ ë³´ê¸°í•  ë ˆì½”ë“œ ì„ íƒ ({id_label})", record_ids)

        if id_label == "id":
            detail_row = df_detail[df_detail["id"].astype(str) == selected_id].iloc[0]
        else:
            detail_row = df_detail[df_detail["_idx"].astype(str) == selected_id].iloc[0]

        st.markdown("### ğŸ” ìƒì„¸ ì •ë³´ (Drill-down)")
        st.json(detail_row.to_dict(), expanded=False)

        score, label, color = compute_risk_score(detail_row)
        st.markdown(f"**ìœ„í—˜ë„:** {color} {label} (score={score})")

        # OSINT Quick Links
        st.markdown("### ğŸŒ OSINT Quick Links")

        WHOIS_URL = "https://www.whois.com/whois/{domain}"
        HIBP_DOMAIN_URL = "https://haveibeenpwned.com/DomainSearch/{domain}"
        DNSDUMPSTER_URL = "https://dnsdumpster.com/"

        domains_str = detail_row.get("domains", "")
        domains = split_csv_list_cell(domains_str)

        if domains:
            for d in domains:
                st.markdown(f"- **{d}**")
                st.markdown(f"  - [Whois ì¡°íšŒ]({WHOIS_URL.format(domain=d)})")
                st.markdown(f"  - [Have I Been Pwned ë„ë©”ì¸ ê²€ìƒ‰]({HIBP_DOMAIN_URL.format(domain=d)})")
                st.markdown(f"  - [DNSDumpster ì—´ê¸°]({DNSDUMPSTER_URL})")
        else:
            st.write("domains ì •ë³´ê°€ ì—†ì–´ OSINT ë§í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV / JSON ë‹¤ìš´ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.markdown("## â¬‡ï¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í˜„ì¬ í•„í„° ê²°ê³¼ ê¸°ì¤€)")

    if df_filtered.empty:
        st.write("ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        df_download = df_filtered.drop(columns=[c for c in df_filtered.columns if c == "_idx"], errors="ignore")

        csv_data = df_download.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name="leak_records_filtered.csv",
            mime="text/csv",
        )

        json_data = df_download.to_json(orient="records", force_ascii=False, indent=2).encode("utf-8")
        st.download_button(
            label="JSON ë‹¤ìš´ë¡œë“œ",
            data=json_data,
            file_name="leak_records_filtered.json",
            mime="application/json",
        )


if __name__ == "__main__":
    main()

# dashboard/app.py

import re
from pathlib import Path
from typing import Tuple

import pandas as pd
import streamlit as st


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Darkweb Leak Intelligence Dashboard",
    layout="wide",
)

st.markdown(
    """
    <style>
    .main > div {
        padding-top: 1rem;
        padding-bottom: 2rem;
    }
    section[data-testid="stSidebar"] > div {
        padding-top: 1rem;
    }
    .small-text {
        font-size: 0.85rem;
        color: #888888;
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ë°ì´í„° íŒŒì¼ ìœ„ì¹˜ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€: /data/leak_records.csv)
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
CSV_PATH = DATA_DIR / "leak_records.csv"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëŒ€ì‹œë³´ë“œ ìŠ¤í‚¤ë§ˆ(ê³ ì •)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# storage.pyì˜ CSV_HEADERì™€ í˜¸í™˜
CSV_HEADER = [
    "source",
    "post_title",
    "target_service",
    "domains",
    "leak_types",
    "estimated_volume",
    "confidence",
    "collected_at",
    "post_id",
    "message_url",
]

# ëŒ€ì‹œë³´ë“œì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œë„ ì“°ëŠ” ì»¬ëŸ¼(ì—†ìœ¼ë©´ ìƒì„±)
DASHBOARD_EXTRA_COLS = [
    "posted_at",   # storage.csvì—ëŠ” ì—†ì„ ìˆ˜ ìˆìŒ â†’ í•­ìƒ ì¡´ì¬í•˜ê²Œ ë§Œë“¤ê¸°
    "threat_claim",
]


def _coerce_str_series(df: pd.DataFrame, col: str) -> None:
    df[col] = df[col].fillna("").astype(str)


def ensure_dashboard_schema(df: pd.DataFrame) -> pd.DataFrame:
    """
    CSVì—ì„œ ë¡œë“œí•œ dfë¥¼ ëŒ€ì‹œë³´ë“œìš©ìœ¼ë¡œ 'í•­ìƒ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆ'ë¡œ ë³´ì •í•œë‹¤.
    - ëˆ„ë½ ì»¬ëŸ¼ ìƒì„±
    - íƒ€ì… ì •ë¦¬(íŠ¹íˆ estimated_volume ìˆ«ì, ë‚ ì§œ íŒŒì‹±)
    - confidence ê¸°ë³¸ê°’ ë³´ì •
    """
    if df is None or df.empty:
        # ë¹ˆ DFë¼ë„ í—¤ë”ëŠ” ê°–ê³  ìˆê²Œ í•´ì„œ ì´í›„ ë¡œì§ì´ ê¹¨ì§€ì§€ ì•Šê²Œ
        df = pd.DataFrame(columns=CSV_HEADER + DASHBOARD_EXTRA_COLS)

    # 1) í•„ìˆ˜ ì»¬ëŸ¼ ìƒì„±
    for col in CSV_HEADER:
        if col not in df.columns:
            df[col] = ""

    # 2) ëŒ€ì‹œë³´ë“œìš© ì¶”ê°€ ì»¬ëŸ¼ ìƒì„±
    for col in DASHBOARD_EXTRA_COLS:
        if col not in df.columns:
            df[col] = ""

    # 3) ë¬¸ìì—´ ì»¬ëŸ¼ ì •ë¦¬
    for col in ["source", "confidence", "post_title", "target_service", "domains", "leak_types", "post_id", "message_url", "threat_claim"]:
        _coerce_str_series(df, col)

    # 4) volume ìˆ«ìí˜• ìºìŠ¤íŒ… (ì •ë ¬/ìœ„í—˜ë„ ê³„ì‚°ìš©)
    df["estimated_volume"] = pd.to_numeric(df["estimated_volume"], errors="coerce")

    # 5) ë‚ ì§œ ìºìŠ¤íŒ… (ì—†ê±°ë‚˜ ì´ìƒí•´ë„ coerce)
    df["collected_at"] = pd.to_datetime(df["collected_at"], errors="coerce")
    df["posted_at"] = pd.to_datetime(df["posted_at"], errors="coerce")

    # 6) confidence ê¸°ë³¸ê°’ (ë¹ˆ ê°’ì´ë©´ medium)
    conf = df["confidence"].str.strip().str.lower()
    df.loc[conf == "", "confidence"] = "medium"

    # 7) í‘œ ì»¬ëŸ¼ ìˆœì„œ ê³ ì •(ê°€ë…ì„± + ë””ë²„ê¹… ìš©ì´)
    ordered_cols = CSV_HEADER + DASHBOARD_EXTRA_COLS
    df = df[ordered_cols]

    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=60)
def load_data() -> pd.DataFrame:
    """CSVì—ì„œ LeakRecord ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤."""
    if not CSV_PATH.exists():
        return ensure_dashboard_schema(pd.DataFrame())

    # dtype=strë¡œ ì½ìœ¼ë©´ NaN/íƒ€ì… í”ë“¤ë¦¼ì´ ì¤„ì–´ë“¦
    df = pd.read_csv(CSV_PATH, dtype=str, keep_default_na=False)
    return ensure_dashboard_schema(df)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ„í—˜ë„ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_risk_score(row: pd.Series) -> Tuple[int, str, str]:
    """
    confidence + estimated_volume ê¸°ë°˜ ìœ„í—˜ë„ ì ìˆ˜/ë¼ë²¨/ì•„ì´ì½˜ ê³„ì‚°.
    ì ìˆ˜(total), ë¼ë²¨, ìƒ‰ìƒì•„ì´ì½˜(ì´ëª¨ì§€) ë°˜í™˜.
    """
    conf = str(row.get("confidence", "")).lower()

    if conf == "high":
        base = 3
    elif conf == "medium":
        base = 2
    elif conf == "low":
        base = 1
    else:
        base = 1

    vol = row.get("estimated_volume", 0)
    if pd.isna(vol):
        vol = 0

    if vol >= 1_000_000:
        vol_score = 3
    elif vol >= 100_000:
        vol_score = 2
    elif vol > 0:
        vol_score = 1
    else:
        vol_score = 0

    total = base + vol_score  # 0 ~ 6

    if total >= 5:
        label = "ìœ„í—˜ë„ ë§¤ìš° ë†’ìŒ"
        color = "ğŸ”´"
    elif total >= 3:
        label = "ìœ„í—˜ë„ ë†’ìŒ"
        color = "ğŸŸ "
    elif total >= 2:
        label = "ìœ„í—˜ë„ ë³´í†µ"
        color = "ğŸŸ¡"
    else:
        label = "ìœ„í—˜ë„ ë‚®ìŒ"
        color = "ğŸŸ¢"

    return total, label, color


def add_risk_info(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì— risk_score / risk_label / risk_indicator ì»¬ëŸ¼ì„ ì¶”ê°€."""
    if df.empty:
        return df

    df = df.copy()
    scores = df.apply(compute_risk_score, axis=1, result_type="expand")
    df["risk_score"] = scores[0]
    df["risk_label"] = scores[1]
    df["risk_indicator"] = scores[2]
    return df


def split_csv_list_cell(value: str) -> list[str]:
    """'a, b, c' í˜•íƒœë¥¼ ['a','b','c']ë¡œ ì•ˆì „í•˜ê²Œ ë¶„ë¦¬."""
    if value is None:
        return []
    s = str(value).strip()
    if not s:
        return []
    return [t.strip() for t in s.split(",") if t.strip()]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    st.title("ğŸŒ™ Darkweb Leak Intelligence Dashboard")
    st.markdown(
        '<p class="small-text">'
        "í…”ë ˆê·¸ë¨ ê¸°ë°˜ ë‹¤í¬ì›¹ ìœ ì¶œ ì •ë³´ ìë™ ìˆ˜ì§‘ Â· ë¶„ì„ ëŒ€ì‹œë³´ë“œ"
        "</p>",
        unsafe_allow_html=True,
    )

    df = load_data()

    if st.button("ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        load_data.clear()
        st.experimental_rerun()

    # ìŠ¤í‚¤ë§ˆê°€ ì´ë¯¸ ë³´ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ df.emptyë§Œ í™•ì¸í•˜ë©´ ë¨
    if df.empty:
        st.warning("í˜„ì¬ data/leak_records.csv íŒŒì¼ì´ ì—†ê±°ë‚˜, ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    df = add_risk_info(df)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‚¬ì´ë“œë°”: ê²€ìƒ‰ Â· í•„í„° Â· ì •ë ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.sidebar.header("ê²€ìƒ‰ / í•„í„°")

    # 1) ê²€ìƒ‰
    st.sidebar.subheader("ê²€ìƒ‰(Search)")
    q_title = st.sidebar.text_input("ì œëª© ê²€ìƒ‰ (post_title)")
    q_domain = st.sidebar.text_input("ë„ë©”ì¸ ê²€ìƒ‰ (domains)")
    q_target = st.sidebar.text_input("íƒ€ê²Ÿ ì„œë¹„ìŠ¤ ê²€ìƒ‰ (target_service)")
    q_source = st.sidebar.text_input("ì†ŒìŠ¤/ì±„ë„ ê²€ìƒ‰ (source)")
    q_any = st.sidebar.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰ (ëª¨ë“  í…ìŠ¤íŠ¸ í•„ë“œ)")

    df_filtered = df.copy()

    if q_title:
        df_filtered = df_filtered[
            df_filtered["post_title"].str.contains(q_title, case=False, na=False)
        ]

    if q_domain:
        df_filtered = df_filtered[
            df_filtered["domains"].str.contains(q_domain, case=False, na=False)
        ]

    if q_target:
        df_filtered = df_filtered[
            df_filtered["target_service"].str.contains(q_target, case=False, na=False)
        ]

    if q_source:
        df_filtered = df_filtered[
            df_filtered["source"].str.contains(q_source, case=False, na=False)
        ]

    if q_any:
        text_cols = df_filtered.select_dtypes(include=["object"]).columns
        if len(text_cols) > 0:
            df_tmp = df_filtered.copy()
            df_tmp["_concat"] = df_tmp[text_cols].fillna("").astype(str).agg(" ".join, axis=1)
            df_filtered = df_tmp[df_tmp["_concat"].str.contains(q_any, case=False, na=False)].drop(columns=["_concat"])

    # 2) í•„í„°
    st.sidebar.subheader("í•„í„°(Filter)")

    # leak_types í•„í„° (í•­ìƒ ì»¬ëŸ¼ ì¡´ì¬)
    all_leak_types: list[str] = []
    for v in df["leak_types"].astype(str):
        all_leak_types.extend(split_csv_list_cell(v))

    leak_type_values = sorted(set(all_leak_types))
    selected_leak_types = st.sidebar.multiselect("Leak Types", leak_type_values)

    if selected_leak_types:
        pattern = "|".join([re.escape(t) for t in selected_leak_types])
        df_filtered = df_filtered[
            df_filtered["leak_types"].astype(str).str.contains(pattern, na=False)
        ]

    # confidence í•„í„° (í•­ìƒ ì»¬ëŸ¼ ì¡´ì¬)
    confidence_values = sorted(df["confidence"].astype(str).str.lower().unique().tolist())
    selected_confidence = st.sidebar.multiselect("Confidence", confidence_values)
    if selected_confidence:
        df_filtered = df_filtered[df_filtered["confidence"].astype(str).str.lower().isin(selected_confidence)]

    # ë‚ ì§œ ë²”ìœ„ í•„í„° (collected_at / posted_at) â€” í•­ìƒ ì¡´ì¬í•˜ê²Œ ë³´ì •ë¨
    st.sidebar.markdown("---")
    st.sidebar.write("ë‚ ì§œ ë²”ìœ„ í•„í„°")

    available_date_fields = ["collected_at", "posted_at"]
    date_field = st.sidebar.selectbox("ê¸°ì¤€ ë‚ ì§œ ì»¬ëŸ¼", available_date_fields, index=0)

    min_date = df[date_field].min()
    max_date = df[date_field].max()

    # ë‚ ì§œê°€ ì „ë¶€ NaTë©´ ë²”ìœ„ ì„ íƒ UI ëŒ€ì‹  ì•ˆë‚´
    if pd.isna(min_date) or pd.isna(max_date):
        st.sidebar.caption("ì„ íƒí•œ ë‚ ì§œ ì»¬ëŸ¼ì— ìœ íš¨í•œ ê°’ì´ ì—†ì–´ ë‚ ì§œ í•„í„°ë¥¼ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        start_date, end_date = st.sidebar.date_input(
            f"{date_field} ë²”ìœ„",
            value=[min_date.date(), max_date.date()],
        )
        if start_date and end_date:
            mask = (df_filtered[date_field] >= pd.to_datetime(start_date)) & (
                df_filtered[date_field] <= pd.to_datetime(end_date)
            )
            df_filtered = df_filtered[mask]

    # 3) ì •ë ¬
    st.sidebar.markdown("---")
    st.sidebar.subheader("ì •ë ¬(Sort)")

    sort_options = [
        "ì •ë ¬ ì—†ìŒ",
        "ìµœì‹ ìˆœ (collected_at desc)",
        "ìµœì‹ ìˆœ (posted_at desc)",
        "volume í° ìˆœ",
        "source ì•ŒíŒŒë²³ ìˆœ",
        "ìœ„í—˜ë„ ë†’ì€ ìˆœ (risk_score desc)",
    ]

    sort_key = st.sidebar.selectbox("ì •ë ¬ ê¸°ì¤€", sort_options)

    if sort_key == "ìµœì‹ ìˆœ (posted_at desc)":
        df_filtered = df_filtered.sort_values("posted_at", ascending=False)
    elif sort_key == "ìµœì‹ ìˆœ (collected_at desc)":
        df_filtered = df_filtered.sort_values("collected_at", ascending=False)
    elif sort_key == "volume í° ìˆœ":
        df_filtered = df_filtered.sort_values("estimated_volume", ascending=False, na_position="last")
    elif sort_key == "source ì•ŒíŒŒë²³ ìˆœ":
        df_filtered = df_filtered.sort_values("source", ascending=True)
    elif sort_key == "ìœ„í—˜ë„ ë†’ì€ ìˆœ (risk_score desc)":
        df_filtered = df_filtered.sort_values("risk_score", ascending=False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìƒë‹¨ KPI ì˜ì—­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì „ì²´ ë ˆì½”ë“œ ìˆ˜", len(df))
    with col2:
        st.metric("í˜„ì¬ í•„í„°ëœ ë ˆì½”ë“œ ìˆ˜", len(df_filtered))
    with col3:
        st.metric("ì†ŒìŠ¤(ì±„ë„) ê°œìˆ˜", df["source"].nunique())

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì‹œê°í™” ì˜ì—­ (matplotlib ì—†ì´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("## ğŸ“ˆ í†µê³„ / ì‹œê°í™”")

    if df_filtered.empty:
        st.info("í˜„ì¬ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        col_g1, col_g2 = st.columns(2)

        with col_g1:
            # ë‚ ì§œë³„ ê±´ìˆ˜ ì¶”ì´: Streamlit line_chart ì‚¬ìš©
            date_field_for_chart = date_field if date_field in ["posted_at", "collected_at"] else "collected_at"

            df_time = df_filtered.copy()
            df_time[date_field_for_chart] = pd.to_datetime(df_time[date_field_for_chart], errors="coerce")
            df_time = df_time.dropna(subset=[date_field_for_chart])

            st.subheader("ë‚ ì§œë³„ ëˆ„ì¶œ ê±´ìˆ˜ ì¶”ì´")
            if not df_time.empty:
                daily_counts = (
                    df_time.groupby(df_time[date_field_for_chart].dt.date)
                    .size()
                    .reset_index(name="count")
                )
                daily_counts = daily_counts.rename(columns={daily_counts.columns[0]: "date"}).set_index("date")
                st.line_chart(daily_counts["count"])
            else:
                st.write("ë‚ ì§œ ì •ë³´ê°€ ì—†ì–´ íŠ¸ë Œë“œ ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with col_g2:
            # ì±„ë„ë³„ ëˆ„ì¶œ ë¹„ì¤‘: Streamlit bar_chart ì‚¬ìš©
            st.subheader("ì±„ë„ë³„ ëˆ„ì¶œ ê±´ìˆ˜ (source ê¸°ì¤€)")
            channel_counts = df_filtered["source"].astype(str).value_counts()
            st.bar_chart(channel_counts)

        # Leak Types ë¹„ìœ¨: bar_chart
        st.subheader("Leak Types (count)")
        all_types: list[str] = []
        for v in df_filtered["leak_types"].astype(str):
            all_types.extend(split_csv_list_cell(v))

        if all_types:
            type_counts = pd.Series(all_types).value_counts()
            st.bar_chart(type_counts)
        else:
            st.write("Leak Types ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë ˆì½”ë“œ í…Œì´ë¸” + ìƒì„¸ ë³´ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("## ğŸ“„ Leak Records")

    # id ì»¬ëŸ¼ì€ ì›ë˜ CSVì— ì—†ìœ¼ë¯€ë¡œ post_idë¥¼ ê¸°ë³¸ í‚¤ë¡œ ì‚¼ëŠ”ë‹¤
    df_table = df_filtered.copy()
    df_table["id"] = df_table["post_id"].astype(str)
    # post_idê°€ ë¹„ì–´ìˆìœ¼ë©´ indexë¥¼ fallback
    missing = df_table["id"].str.strip() == ""
    if missing.any():
        df_table.loc[missing, "id"] = df_table.loc[missing].reset_index()["index"].astype(str).values

    columns_for_table = [
        "id",
        "risk_indicator",
        "risk_label",
        "confidence",
        "estimated_volume",
        "post_title",
        "source",
        "domains",
        "leak_types",
        "posted_at",
        "collected_at",
    ]
    # í˜„ì¬ df_tableì—ëŠ” ëª¨ë‘ ì¡´ì¬(ensure_dashboard_schema + add_risk_infoë¡œ ë³´ì¥)
    st.dataframe(df_table[columns_for_table], use_container_width=True, hide_index=True)

    record_ids = df_table["id"].astype(str).tolist()
    if record_ids:
        selected_id = st.selectbox("ìƒì„¸ ë³´ê¸°í•  ë ˆì½”ë“œ ì„ íƒ (id)", record_ids)

        detail_row = df_table[df_table["id"].astype(str) == selected_id].iloc[0]

        st.markdown("### ğŸ” ìƒì„¸ ì •ë³´ (Drill-down)")
        st.json(detail_row.to_dict(), expanded=False)

        score, label, color = compute_risk_score(detail_row)
        st.markdown(f"**ìœ„í—˜ë„:** {color} {label} (score={score})")

        # OSINT Quick Links
        st.markdown("### ğŸŒ OSINT Quick Links")

        WHOIS_URL = "https://www.whois.com/whois/{domain}"
        HIBP_DOMAIN_URL = "https://haveibeenpwned.com/DomainSearch/{domain}"
        DNSDUMPSTER_URL = "https://dnsdumpster.com/"

        domains_str = detail_row.get("domains", "")
        domains = split_csv_list_cell(domains_str)

        if domains:
            for d in domains:
                st.markdown(f"- **{d}**")
                st.markdown(f"  - [Whois ì¡°íšŒ]({WHOIS_URL.format(domain=d)})")
                st.markdown(f"  - [Have I Been Pwned ë„ë©”ì¸ ê²€ìƒ‰]({HIBP_DOMAIN_URL.format(domain=d)})")
                st.markdown(f"  - [DNSDumpster ì—´ê¸°]({DNSDUMPSTER_URL})")
        else:
            # â€œì—†ì–´ì„œ ìƒì„± ë¶ˆê°€â€ë¥¼ ì—ëŸ¬ì²˜ëŸ¼ ë³´ì´ì§€ ì•Šê²Œ ì²˜ë¦¬
            st.caption("ë„ë©”ì¸ ì •ë³´ê°€ ì—†ëŠ” ë ˆì½”ë“œì…ë‹ˆë‹¤. (OSINT ë§í¬ ìƒëµ)")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV / JSON ë‹¤ìš´ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.markdown("## â¬‡ï¸ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í˜„ì¬ í•„í„° ê²°ê³¼ ê¸°ì¤€)")

    if df_filtered.empty:
        st.write("ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        df_download = df_filtered.copy()

        csv_data = df_download.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name="leak_records_filtered.csv",
            mime="text/csv",
        )

        json_data = df_download.to_json(orient="records", force_ascii=False, indent=2).encode("utf-8")
        st.download_button(
            label="JSON ë‹¤ìš´ë¡œë“œ",
            data=json_data,
            file_name="leak_records_filtered.json",
            mime="application/json",
        )


if __name__ == "__main__":
    main()
